We described a virtual resource model, and a set of scheduling
strategies for that model, based on scenarios that, in our experience,
arise frequently in the Grid, and which can involve best--effort deployments as well as
deadline{}-sensitive deployments. VM preparation and
runtime overhead can be both large and highly variable, factors that
conflict with the deadline{}-sensitive availability needs of
interactive and time{}-critical platforms. Thus, our proposed model
separates resource use devoted to the overhead of VM deployment from
resources available to the VM itself, enabling us to schedule overhead
resource slots equally with VM slots.

Our results show that using workspace metadata and overhead scheduling, in accordance with our model, results in improved accuracy and efficiency. Providing the scheduler with information about the VM images needed by a virtual workspace can have two benefits. First, the required transfer operations can be scheduled in advance, resulting in better adherence to requested availability time. Second, we can use information about a VM image as defined in the workspace metadata to optimize the resource usage devoted to VM deployment by reusing VM images and thus reducing the number of image transfers. Both strategies have benefits for the resource provider and for the client in workloads requiring a large number of image transfers, and were predeployment is not a possibility. Furthermore, by reducing preparation overhead, these strategies also make the deployment of short{}-lived VMs more cost{}-effective.

Our results also show that leveraging VM resource management mechanisms, such as suspend/resume, can result in improved utilization of resources, specially in the presence of long jobs where it becomes difficult to backfill the time before a reservation.

\section{Future work}

Our future work on this subject will involve developing models that provide accurate and fine-grained use of other resources, such as CPU, memory, network bandwidth. Network bandwidth, in particular, presents interesting questions, since network traffic affects the CPU usage of Dom0, Xen's management domain. Therefore, running network-intensive VMs requires ensuring that Dom0 always has enough CPU share to provide all the VMs with the network bandwidth they require. Future models must be able to  compute the CPU share required by Dom0 dynamically, under varying loads, guaranteeing that bandwidth will be available for all the VMs, while Dom0 is not be strained for resources. Disk usage is another interesting dimension which we have touched upon in this work, but which requires further exploration to account for different deployment strategies, multi--partition VMs, and different storage backends.

We will also explore Open Advance Reservations, to support event--driven workloads. As described in Section~\ref{cha:scenarios}, this scenario requires that resources be available when an event arrives. Even though this event arrives during an agreed-upon period of time, the exact time of the event is unknown. Our model must allow for resource to be placed on standby (in preparation for that event) without affecting other virtual workspaces, or wasting resources that could be used before the event arrives.

On the implementation front, we plan to use real submission traces for our mixed workload (best--effort and advance reservation) experiments, and transition from a simulated backend to a real backend. We will also explore how to include parts of our scheduler into the Virtual Workspace Service, and evaluate existing local resource managers where our scheduling techniques could be integrated.