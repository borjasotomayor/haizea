We described a virtual resource model, and a set of scheduling
strategies for that model, based on scenarios that, in our experience,
arise frequently in the Grid, and which can involve best--effort deployments as well as
deadline{}-sensitive deployments. VM preparation and
runtime overhead can be both large and highly variable, factors that
conflict with the deadline{}-sensitive availability needs of
interactive and time{}-critical platforms. Thus, our proposed model
separates resource use devoted to the overhead of VM deployment from
resources available to the VM itself, enabling us to schedule overhead
resource slots equally with VM slots.

We have shown experimental results that evaluate our resource model from three different perspectives:

\begin{itemize}
\item[---] \emph{Accuracy}: Our first set of experiments investigated the effect of VM image transfers on accuracy of AR deployments. By providing the scheduler with information on the VM image required for each request, thus allowing the preparation overhead of the image transfer to be scheduled with a deadline--sensitive algorithm (EDF), our system was able to guarantee that VM images were available before the start time of an AR request. Non--scheduled file staging strategies resulted in worse accuracy, with the user getting only 55\% of the requested time in the worst case.
\item[---] \emph{Efficiency}: Our second set of experiments show to what extent image reuse, in best--effort deployments, can improve network bandwidth utilization and latency of requests. By reusing VM images on the physical nodes, less image transfers are required, which allows the resource provider to use the saved bandwidth to process more requests, and reducing the average time to deploy a single image by as much as 44\%.
\item[---] \emph{Mixing Best--Effort requests and Advance Reservations}: Our final set of results provide some insight into the benefits and drawbacks of using VMs for mixed workloads, containing both best--effort requests and ARs. We observed that, by using the suspend/resume capabilities of VMs, the running time of mixed workloads could be reduced by up to $9.5\%$, assuming that all necessary VM images could be predeployed. Even in the worst--case, when our scheduling techniques could not sufficiently compensate for the runtime overhead of VMs, the running time was increased by no more that $8.1\%$. Assuming that predeployment of images is not possible, we observed that preparation overhead can have a noticeable impact on performance, increasing the running time of the workload, when compared to running with predeployed images, by as much as $97.34\%$. This value is observed in an execution trace that requires the deployment of more VM images than the available network bandwidth could handle. However, Using image reuse strategies brings this worst case down to a more reasonable $4.9\%$ increase in running time. In general, removing the assumption that images could be predeployed, but using image reuse strategies, had no effect on performance in the best case, and only a $7.3\%$ impact in the worst case.
\end{itemize}

From these results we can conclude that using workspace metadata and overhead scheduling, in accordance with our model, results in improved accuracy and efficiency. Providing the scheduler with information about the VM images needed by a virtual workspace can have two benefits. First, the required transfer operations can be scheduled in advance, resulting in better adherence to requested availability time. Second, we can use information about a VM image as defined in the workspace metadata to optimize the resource usage devoted to VM deployment by reusing VM images and thus reducing the number of image transfers. Both strategies have benefits for the resource provider and for the client in workloads requiring a large number of image transfers, and where predeployment is not a possibility. Furthermore, by reducing preparation overhead, these strategies also make the deployment of short{}-lived VMs more cost{}-effective. Our results also show that leveraging VM resource management mechanisms, such as suspend/resume, can result in improved utilization of resources, specially in the presence of long jobs where it becomes difficult to backfill the time before a reservation.


\section{Future work}

Our future work on this subject will involve developing models that provide accurate and fine-grained use of other resources, such as CPU, memory, network bandwidth. Network bandwidth, in particular, presents interesting questions, since network traffic affects the CPU usage of Dom0, Xen's management domain. Therefore, running network-intensive VMs requires ensuring that Dom0 always has enough CPU share to provide all the VMs with the network bandwidth they require. Future models must be able to  compute the CPU share required by Dom0 dynamically, under varying loads, guaranteeing that bandwidth will be available for all the VMs, while Dom0 is not be strained for resources. Disk usage is another interesting dimension which we have touched upon in this work, but which requires further exploration to account for different deployment strategies, multi--partition VMs, and different storage backends.

We will also explore Open Advance Reservations, to support event--driven workloads. As described in Section~\ref{cha:scenarios}, this scenario requires that resources be available when an event arrives. Even though this event arrives during an agreed-upon period of time, the exact time of the event is unknown. Our model must allow for resource to be placed on standby (in preparation for that event) without affecting other virtual workspaces, or wasting resources that could be used before the event arrives.

On the implementation front, we plan to use real submission traces for our mixed workload (best--effort and advance reservation) experiments, and transition from a simulated backend to a real backend. We will also explore how to include parts of our scheduler into the Virtual Workspace Service, and evaluate existing local resource managers where our scheduling techniques could be integrated.
